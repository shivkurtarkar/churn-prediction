{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client link:\n",
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Set up a Dask Cluster\n",
    "cluster = LocalCluster(n_workers=6, threads_per_worker=1, memory_limit='18GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"client link:\")\n",
    "print(client.dashboard_link)  # Clickable link to the dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Variables for filepaths\n",
    "DATA_DIR=\"../../data\"\n",
    "\n",
    "MEMBERS_FILE=f\"{DATA_DIR}/members_v3.csv\"\n",
    "TRANSACTION_FILE=f\"{DATA_DIR}/transactions.csv\"\n",
    "TRAIN_FILE=f\"{DATA_DIR}/train.csv\"\n",
    "USERLOG_FILE=f\"{DATA_DIR}/user_logs.csv\"\n",
    "SAMPLE_SUBMISSION_FILE=f\"{DATA_DIR}/sample_submission_zero.csv\"\n",
    "\n",
    "TRANSACTION_V2_FILE=f\"{DATA_DIR}/transactions_v2.csv\"\n",
    "TRAIN_V2_FILE=f\"{DATA_DIR}/train_v2.csv\"\n",
    "USER_LOGS_V2_FILE=f\"{DATA_DIR}/user_logs_v2.csv\"\n",
    "SAMPLE_SUBMISSION_V2_FILE=f\"{DATA_DIR}/sample_submission_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: ../../data\n",
      "TRANSACTION_FILE: ../../data/transactions.csv\n",
      "USERLOG_FILE: ../../data/user_logs.csv\n",
      "TRAIN_FILE: ../../data/train.csv\n",
      "SAMPLE_SUBMISSION_FILE: ../../data/sample_submission_zero.csv\n",
      "MEMBERS_FILE: ../../data/members_v3.csv\n",
      "\n",
      "TRANSACTION_V2_FILE: ../../data/transactions_v2.csv\n",
      "USER_LOGS_V2_FILE: ../../data/user_logs_v2.csv\n",
      "TRAIN_V2_FILE: ../../data/train_v2.csv\n",
      "SAMPLE_SUBMISSION_V2_FILE: ../../data/sample_submission_v2.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"TRANSACTION_FILE: {TRANSACTION_FILE}\")\n",
    "print(f\"USERLOG_FILE: {USERLOG_FILE}\")\n",
    "print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n",
    "print(f\"SAMPLE_SUBMISSION_FILE: {SAMPLE_SUBMISSION_FILE}\")\n",
    "print(f\"MEMBERS_FILE: {MEMBERS_FILE}\")\n",
    "print()\n",
    "print(f\"TRANSACTION_V2_FILE: {TRANSACTION_V2_FILE}\")\n",
    "print(f\"USER_LOGS_V2_FILE: {USER_LOGS_V2_FILE}\")\n",
    "print(f\"TRAIN_V2_FILE: {TRAIN_V2_FILE}\")\n",
    "print(f\"SAMPLE_SUBMISSION_V2_FILE: {SAMPLE_SUBMISSION_V2_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dd.read_csv(TRAIN_FILE)\n",
    "# train = dd.concat((train, dd.read_csv(TRAIN_V2_FILE)), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "train = dd.concat([train, dd.read_csv(TRAIN_V2_FILE)])\n",
    "\n",
    "test = dd.read_csv(SAMPLE_SUBMISSION_V2_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963891"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 17:20:52,551 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 3974414222adaab51ab6d25d14c87300 initialized by task ('shuffle-transfer-3974414222adaab51ab6d25d14c87300', 25) executed on worker tcp://127.0.0.1:42637\n",
      "2025-01-12 17:21:14,759 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 3974414222adaab51ab6d25d14c87300 deactivated due to stimulus 'task-finished-1736682674.7568426'\n",
      "2025-01-12 17:21:25,780 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dad11b8a683af2c796ead39df44d60e0 initialized by task ('shuffle-transfer-dad11b8a683af2c796ead39df44d60e0', 17) executed on worker tcp://127.0.0.1:45093\n",
      "2025-01-12 17:21:40,903 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle dad11b8a683af2c796ead39df44d60e0 deactivated due to stimulus 'task-finished-1736682700.9005084'\n",
      "2025-01-12 17:21:51,367 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 22148b1fba37f14d48ae059f2c632237 initialized by task ('hash-join-transfer-22148b1fba37f14d48ae059f2c632237', 3) executed on worker tcp://127.0.0.1:45093\n",
      "2025-01-12 17:21:55,534 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 141ab4c80fe5bfaa18f49e45205f5cf7 initialized by task ('shuffle-transfer-141ab4c80fe5bfaa18f49e45205f5cf7', 0) executed on worker tcp://127.0.0.1:42637\n",
      "2025-01-12 17:21:57,872 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 141ab4c80fe5bfaa18f49e45205f5cf7 deactivated due to stimulus 'task-finished-1736682717.870489'\n",
      "2025-01-12 17:22:03,293 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 7d0ceb06ecaf32566689ce39166abd07 initialized by task ('hash-join-transfer-7d0ceb06ecaf32566689ce39166abd07', 0) executed on worker tcp://127.0.0.1:44393\n",
      "2025-01-12 17:22:09,162 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 7d0ceb06ecaf32566689ce39166abd07 deactivated due to stimulus 'task-finished-1736682729.15733'\n",
      "2025-01-12 17:22:09,163 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 22148b1fba37f14d48ae059f2c632237 deactivated due to stimulus 'task-finished-1736682729.15733'\n",
      "2025-01-12 17:22:19,177 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 91381a5f6e0a2c87c222e1f1ac0cf16c initialized by task ('hash-join-transfer-91381a5f6e0a2c87c222e1f1ac0cf16c', 1) executed on worker tcp://127.0.0.1:45665\n",
      "2025-01-12 17:22:25,713 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 141ab4c80fe5bfaa18f49e45205f5cf7 initialized by task ('shuffle-transfer-141ab4c80fe5bfaa18f49e45205f5cf7', 0) executed on worker tcp://127.0.0.1:33701\n",
      "2025-01-12 17:22:26,813 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 141ab4c80fe5bfaa18f49e45205f5cf7 deactivated due to stimulus 'task-finished-1736682746.8121607'\n",
      "2025-01-12 17:22:31,609 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 283bd926237262300a430dedc2c18edc initialized by task ('hash-join-transfer-283bd926237262300a430dedc2c18edc', 0) executed on worker tcp://127.0.0.1:44393\n",
      "2025-01-12 17:22:35,028 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 91381a5f6e0a2c87c222e1f1ac0cf16c deactivated due to stimulus 'task-finished-1736682755.025277'\n",
      "2025-01-12 17:22:35,029 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 283bd926237262300a430dedc2c18edc deactivated due to stimulus 'task-finished-1736682755.025277'\n"
     ]
    }
   ],
   "source": [
    "transactions = dd.read_csv(TRANSACTION_FILE, usecols=['msno'])\n",
    "transactions = dd.concat([transactions, dd.read_csv(TRANSACTION_V2_FILE, usecols=['msno'])])\n",
    "transactions = dd.DataFrame(transactions['msno'].value_counts().reset_index()).compute()\n",
    "transactions.columns = ['msno','trans_count']\n",
    "train = train.merge(transactions, how='left', on='msno')\n",
    "test = test.merge(transactions, how='left', on='msno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv(TRANSACTION_V2_FILE) \n",
    "transactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\n",
    "transactions = transactions.drop_duplicates(subset=['msno'], keep='first')\n",
    "\n",
    "train = dd.merge(train, transactions, how='left', on='msno')\n",
    "test = dd.merge(test, transactions, how='left', on='msno')\n",
    "# transactions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = dd.read_csv(USER_LOGS_V2_FILE, usecols=['msno'])\n",
    "user_logs = dd.DataFrame(user_logs['msno'].value_counts().reset_index()).compute()\n",
    "user_logs.columns = ['msno','logs_count']\n",
    "train = dd.merge(train, user_logs, how='left', on='msno')\n",
    "test = dd.merge(test, user_logs, how='left', on='msno')\n",
    "# user_logs = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>trans_count</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "      <th>logs_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64[pyarrow]</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64[pyarrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: merge, 15 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 msno is_churn     trans_count payment_method_id payment_plan_days plan_list_price actual_amount_paid is_auto_renew transaction_date membership_expire_date is_cancel      logs_count\n",
       "npartitions=2                                                                                                                                                                                        \n",
       "               string    int64  int64[pyarrow]             int64             int64           int64              int64         int64            int64                  int64     int64  int64[pyarrow]\n",
       "                  ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "                  ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "Dask Name: merge, 15 expressions\n",
       "Expr=Merge(599b508)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    df = dd.DataFrame(df)\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df\n",
    "\n",
    "def transform_df2(df):\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_user_logs = []\n",
    "# last_user_logs.append(transform_df(dd.read_csv(USER_LOGS_V2_FILE)))\n",
    "# last_user_logs = dd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "# last_user_logs = transform_df2(last_user_logs)\n",
    "# print ('merging user logs features...')\n",
    "# train = dd.merge(train, last_user_logs, how='left', on='msno')\n",
    "# test = dd.merge(test, last_user_logs, how='left', on='msno')\n",
    "# # last_user_logs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "members merge...\n"
     ]
    }
   ],
   "source": [
    "members = dd.read_csv(MEMBERS_FILE)\n",
    "train = dd.merge(train, members, how='left', on='msno')\n",
    "test = dd.merge(test, members, how='left', on='msno')\n",
    "print('members merge...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/dask_expr/_collection.py:4225: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('gender', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/dask_expr/_collection.py:4225: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('gender', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "gender = {'male':1, 'female':2}\n",
    "train['gender'] = train['gender'].map(gender)\n",
    "test['gender'] = test['gender'].map(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 228.95 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 228.95 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = train.compute()\n",
    "test = test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del members\n",
    "del transactions\n",
    "del user_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>trans_count</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "      <th>logs_count</th>\n",
       "      <th>city</th>\n",
       "      <th>bd</th>\n",
       "      <th>gender</th>\n",
       "      <th>registered_via</th>\n",
       "      <th>registration_init_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20050406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nb1ZGEmagQeba5E+nQj8VlQoWl+8SFmLZu+Y8ytIamw=</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170307.0</td>\n",
       "      <td>20170406.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20060826.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I8dFN2EjFN1mt4Xel8WQX1/g7u6Dg4PBMHLkiDjhUS8=</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>38.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170322.0</td>\n",
       "      <td>20170421.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20061222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+THH2QTeGyADYlZvoaYUXCyoS1iLQsHq59ElGxwwGlE=</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>38.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>894.0</td>\n",
       "      <td>894.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20170216.0</td>\n",
       "      <td>20170831.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20080515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ngHjqujoWJdkjMy+0t8IATYeN2NAhN/yIYszLXAyfSc=</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20090105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328792</th>\n",
       "      <td>PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20170318.0</td>\n",
       "      <td>20170417.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20110812.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328793</th>\n",
       "      <td>MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20170318.0</td>\n",
       "      <td>20170419.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20110817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328794</th>\n",
       "      <td>SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>39.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20170331.0</td>\n",
       "      <td>20170520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20110823.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328795</th>\n",
       "      <td>eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20170321.0</td>\n",
       "      <td>20170422.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20110830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328796</th>\n",
       "      <td>/QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>41.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20170317.0</td>\n",
       "      <td>20170417.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20110830.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1963891 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                msno  is_churn  trans_count  \\\n",
       "0       waLDQMmcOu2jLDaV1ddDkgCrB/jl6sD66Xzs0Vqax1Y=         1            2   \n",
       "1       Nb1ZGEmagQeba5E+nQj8VlQoWl+8SFmLZu+Y8ytIamw=         1           23   \n",
       "2       I8dFN2EjFN1mt4Xel8WQX1/g7u6Dg4PBMHLkiDjhUS8=         1           27   \n",
       "3       +THH2QTeGyADYlZvoaYUXCyoS1iLQsHq59ElGxwwGlE=         1            5   \n",
       "4       ngHjqujoWJdkjMy+0t8IATYeN2NAhN/yIYszLXAyfSc=         1           24   \n",
       "...                                              ...       ...          ...   \n",
       "328792  PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=         0           27   \n",
       "328793  MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=         0           28   \n",
       "328794  SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=         0           21   \n",
       "328795  eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=         0           28   \n",
       "328796  /QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=         0           31   \n",
       "\n",
       "        payment_method_id  payment_plan_days  plan_list_price  \\\n",
       "0                     0.0                0.0              0.0   \n",
       "1                    38.0               30.0            149.0   \n",
       "2                    38.0               30.0            149.0   \n",
       "3                    38.0              195.0            894.0   \n",
       "4                     0.0                0.0              0.0   \n",
       "...                   ...                ...              ...   \n",
       "328792               40.0               30.0            149.0   \n",
       "328793               41.0               30.0            100.0   \n",
       "328794               39.0               30.0            149.0   \n",
       "328795               41.0               30.0             99.0   \n",
       "328796               41.0               30.0            149.0   \n",
       "\n",
       "        actual_amount_paid  is_auto_renew  transaction_date  \\\n",
       "0                      0.0            0.0               0.0   \n",
       "1                    149.0            0.0        20170307.0   \n",
       "2                    149.0            0.0        20170322.0   \n",
       "3                    894.0            0.0        20170216.0   \n",
       "4                      0.0            0.0               0.0   \n",
       "...                    ...            ...               ...   \n",
       "328792               149.0            1.0        20170318.0   \n",
       "328793               100.0            1.0        20170318.0   \n",
       "328794               149.0            1.0        20170331.0   \n",
       "328795                99.0            1.0        20170321.0   \n",
       "328796               149.0            1.0        20170317.0   \n",
       "\n",
       "        membership_expire_date  is_cancel  logs_count  city    bd  gender  \\\n",
       "0                          0.0        0.0           0  18.0  36.0     2.0   \n",
       "1                   20170406.0        0.0          22  18.0  22.0     2.0   \n",
       "2                   20170421.0        0.0          30   4.0  43.0     1.0   \n",
       "3                   20170831.0        0.0          26  17.0  28.0     1.0   \n",
       "4                          0.0        0.0           0  22.0  37.0     2.0   \n",
       "...                        ...        ...         ...   ...   ...     ...   \n",
       "328792              20170417.0        0.0          31   4.0  28.0     1.0   \n",
       "328793              20170419.0        0.0           4   1.0   0.0     0.0   \n",
       "328794              20170520.0        0.0          21  15.0  19.0     1.0   \n",
       "328795              20170422.0        0.0          31  18.0   0.0     2.0   \n",
       "328796              20170417.0        0.0          24   8.0  25.0     2.0   \n",
       "\n",
       "        registered_via  registration_init_time  \n",
       "0                  9.0              20050406.0  \n",
       "1                  9.0              20060826.0  \n",
       "2                  9.0              20061222.0  \n",
       "3                  9.0              20080515.0  \n",
       "4                  9.0              20090105.0  \n",
       "...                ...                     ...  \n",
       "328792             9.0              20110812.0  \n",
       "328793             7.0              20110817.0  \n",
       "328794             9.0              20110823.0  \n",
       "328795             7.0              20110830.0  \n",
       "328796             7.0              20110830.0  \n",
       "\n",
       "[1963891 rows x 17 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ c for c in train.columns if c not in ['is_churn', 'msno']]\n",
    "\n",
    "X_train = train[cols]\n",
    "y_train = train['is_churn']\n",
    "X_test = test[cols]\n",
    "# y_train = train['is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963891"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Feed Forward Network -- \n",
      "Training Accuracy: 0.9232\n",
      "Training Log Loss: 0.2709\n"
     ]
    }
   ],
   "source": [
    "mean_is_churn = train['is_churn'].mean()\n",
    "\n",
    "print(f\"-- Feed Forward Network -- \")\n",
    "# Evaluate the model\n",
    "y_pred_prob = [mean_is_churn]*X_train.shape[0]\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple feedforward neural netwok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Feed Forward Network -- \n",
      "Training Accuracy: 0.9590\n",
      "Training Log Loss: 0.1085\n"
     ]
    }
   ],
   "source": [
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Define the MLP model in scikit-learn\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # Layer sizes similar to Keras\n",
    "    activation='relu',\n",
    "    solver='adam',                 # Using 'adadelta' optimizer\n",
    "    alpha=0.1,                         # L2 regularization\n",
    "    max_iter=200,                      # Number of iterations for training\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(f\"-- Feed Forward Network -- \")\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_train_scaled)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_train_scaled)\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Decision Tree -- \n",
      "Training Accuracy: 0.9638\n",
      "Training Log Loss: 0.1110\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the model\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"-- Decision Tree -- \")\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_train_scaled)\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 150801, number of negative: 1813090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.152223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 968\n",
      "[LightGBM] [Info] Number of data points in the train set: 1963891, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076787 -> initscore=-2.486827\n",
      "[LightGBM] [Info] Start training from score -2.486827\n",
      "-- LightGBM --\n",
      "Training Accuracy: 0.9664\n",
      "Training Log Loss: 0.0745\n"
     ]
    }
   ],
   "source": [
    "# Prepare LightGBM dataset\n",
    "# Ensure your dataframe is named appropriately\n",
    "X_train_lgb = X_train.copy()\n",
    "X_train_lgb['trans_count'] = X_train_lgb['trans_count'].astype('int64')\n",
    "X_train_lgb['logs_count'] = X_train_lgb['logs_count'].astype('int64')\n",
    "\n",
    "train_data = lgb.Dataset(X_train_lgb, label=y_train)\n",
    "# test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',          # For binary classification\n",
    "    'boosting_type': 'gbdt',        # Gradient Boosting Decision Tree\n",
    "    'metric': 'binary_logloss',     # Loss metric\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lgb_model = lgb.train(params, train_data, num_boost_round=100)\n",
    "# lgb_model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=100, early_stopping_rounds=10)\n",
    "\n",
    "# Predict\n",
    "y_pred = lgb_model.predict(X_train_lgb)\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "print(\"-- LightGBM --\")\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred_binary)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using LightGBM\n",
    "y_pred_prob = lgb_model.predict(X_train_lgb)  # This gives probabilities directly\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- XGBoost -- \n",
      "Training Accuracy: 0.9656\n",
      "Training Log Loss: 0.0794\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_train)\n",
    "\n",
    "print(f\"-- XGBoost -- \")\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using XGBoost\n",
    "y_pred_prob = xgb_model.predict_proba(X_train)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # Prepare the LightGBM model\n",
    "# lgb_estimator = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', random_state=42)\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'num_leaves': [31, 50, 100],         # Increasing leaves can improve accuracy\n",
    "#     'max_depth': [-1, 10, 20],          # -1 means no limit\n",
    "#     'learning_rate': [0.01, 0.05, 0.1], # Smaller rates need more boosting rounds\n",
    "#     'n_estimators': [50, 100, 200],     # Number of boosting rounds\n",
    "#     'min_child_samples': [10, 20, 30],  # Minimum number of data points per leaf\n",
    "# }\n",
    "\n",
    "# # Perform Grid Search\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=lgb_estimator,\n",
    "#     param_grid=param_grid,\n",
    "#     scoring='neg_log_loss',  # Minimize log loss\n",
    "#     cv=3,                    # 3-fold cross-validation\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Fit the model\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and score\n",
    "# print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best Log Loss: {-grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:02,866] A new study created in memory with name: no-name-ef2e0287-315a-429f-ade8-e8c5082b2145\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:11,369] Trial 0 finished with value: 0.1105862790975042 and parameters: {'learning_rate': 0.011542297519835237, 'num_leaves': 28, 'min_child_samples': 29, 'n_estimators': 100, 'max_depth': 14}. Best is trial 0 with value: 0.1105862790975042.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:15,819] Trial 1 finished with value: 0.09580206312960858 and parameters: {'learning_rate': 0.07078417077243734, 'num_leaves': 38, 'min_child_samples': 17, 'n_estimators': 79, 'max_depth': 2}. Best is trial 1 with value: 0.09580206312960858.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.053657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:27,020] Trial 2 finished with value: 0.101063503919151 and parameters: {'learning_rate': 0.011838105692582675, 'num_leaves': 31, 'min_child_samples': 20, 'n_estimators': 122, 'max_depth': 12}. Best is trial 1 with value: 0.09580206312960858.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:34,111] Trial 3 finished with value: 0.07990792684407444 and parameters: {'learning_rate': 0.05037437577658824, 'num_leaves': 37, 'min_child_samples': 29, 'n_estimators': 67, 'max_depth': 14}. Best is trial 3 with value: 0.07990792684407444.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055936 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:42,162] Trial 4 finished with value: 0.08714925741432154 and parameters: {'learning_rate': 0.027282687515298262, 'num_leaves': 36, 'min_child_samples': 27, 'n_estimators': 81, 'max_depth': -1}. Best is trial 3 with value: 0.07990792684407444.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:48,349] Trial 5 finished with value: 0.1302206816268464 and parameters: {'learning_rate': 0.012651340204080729, 'num_leaves': 45, 'min_child_samples': 29, 'n_estimators': 59, 'max_depth': -1}. Best is trial 3 with value: 0.07990792684407444.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050643 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:31:52,952] Trial 6 finished with value: 0.12674323385763728 and parameters: {'learning_rate': 0.03482486262015551, 'num_leaves': 50, 'min_child_samples': 27, 'n_estimators': 97, 'max_depth': 1}. Best is trial 3 with value: 0.07990792684407444.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056557 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:03,514] Trial 7 finished with value: 0.08690157528214329 and parameters: {'learning_rate': 0.019593585090576026, 'num_leaves': 50, 'min_child_samples': 23, 'n_estimators': 115, 'max_depth': 8}. Best is trial 3 with value: 0.07990792684407444.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:12,018] Trial 8 finished with value: 0.07668946969166358 and parameters: {'learning_rate': 0.08309885623140148, 'num_leaves': 28, 'min_child_samples': 19, 'n_estimators': 90, 'max_depth': -1}. Best is trial 8 with value: 0.07668946969166358.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:23,588] Trial 9 finished with value: 0.07567796241755222 and parameters: {'learning_rate': 0.06962442021440195, 'num_leaves': 35, 'min_child_samples': 23, 'n_estimators': 138, 'max_depth': 12}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:34,443] Trial 10 finished with value: 0.07618975403474569 and parameters: {'learning_rate': 0.09824522445747652, 'num_leaves': 21, 'min_child_samples': 11, 'n_estimators': 149, 'max_depth': 8}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:44,972] Trial 11 finished with value: 0.07632902960525458 and parameters: {'learning_rate': 0.09579613353391302, 'num_leaves': 20, 'min_child_samples': 10, 'n_estimators': 150, 'max_depth': 9}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:32:55,892] Trial 12 finished with value: 0.07839419882669009 and parameters: {'learning_rate': 0.058512426874154956, 'num_leaves': 22, 'min_child_samples': 10, 'n_estimators': 150, 'max_depth': 5}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:33:07,865] Trial 13 finished with value: 0.07653442756337114 and parameters: {'learning_rate': 0.04350501235111091, 'num_leaves': 42, 'min_child_samples': 14, 'n_estimators': 128, 'max_depth': 11}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:33:17,892] Trial 14 finished with value: 0.07782196249564767 and parameters: {'learning_rate': 0.07264846147641425, 'num_leaves': 24, 'min_child_samples': 24, 'n_estimators': 136, 'max_depth': 5}. Best is trial 9 with value: 0.07567796241755222.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:33:28,604] Trial 15 finished with value: 0.07531796275663362 and parameters: {'learning_rate': 0.09496788635642363, 'num_leaves': 31, 'min_child_samples': 15, 'n_estimators': 135, 'max_depth': 10}. Best is trial 15 with value: 0.07531796275663362.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:33:38,282] Trial 16 finished with value: 0.07670540158597214 and parameters: {'learning_rate': 0.05901126718576082, 'num_leaves': 32, 'min_child_samples': 15, 'n_estimators': 111, 'max_depth': 11}. Best is trial 15 with value: 0.07531796275663362.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058504 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:33:51,732] Trial 17 finished with value: 0.07683096868587785 and parameters: {'learning_rate': 0.037501415558769784, 'num_leaves': 41, 'min_child_samples': 23, 'n_estimators': 138, 'max_depth': 15}. Best is trial 15 with value: 0.07531796275663362.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:34:04,133] Trial 18 finished with value: 0.07963680252545714 and parameters: {'learning_rate': 0.027215872426379448, 'num_leaves': 33, 'min_child_samples': 14, 'n_estimators': 135, 'max_depth': 10}. Best is trial 15 with value: 0.07531796275663362.\n",
      "/tmp/ipykernel_4258/2215912276.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 120524, number of negative: 1450588\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050186 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 964\n",
      "[LightGBM] [Info] Number of data points in the train set: 1571112, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076713 -> initscore=-2.487875\n",
      "[LightGBM] [Info] Start training from score -2.487875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-12 20:34:13,579] Trial 19 finished with value: 0.07770309178062951 and parameters: {'learning_rate': 0.06576323074456261, 'num_leaves': 29, 'min_child_samples': 20, 'n_estimators': 112, 'max_depth': 6}. Best is trial 15 with value: 0.07531796275663362.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.09496788635642363, 'num_leaves': 31, 'min_child_samples': 15, 'n_estimators': 135, 'max_depth': 10}\n",
      "Best Log Loss: 0.0753\n",
      "[LightGBM] [Info] Number of positive: 150801, number of negative: 1813090\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 968\n",
      "[LightGBM] [Info] Number of data points in the train set: 1963891, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076787 -> initscore=-2.486827\n",
      "[LightGBM] [Info] Start training from score -2.486827\n",
      "Final Training Log Loss: 0.0735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define a reduced hyperparameter search space\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),                 # Reduced range\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 30),   # Smaller range\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),            # Moderate boosting rounds\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),                   # Practical range\n",
    "    }\n",
    "\n",
    "    # Split data for validation\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
    "        X_train_lgb, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    # model = model.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    model.fit(X_train_part, y_train_part, eval_set=[(X_valid, y_valid)])\n",
    "            #   eval_metric='logloss',  verbose=0) # early_stopping_rounds=10,\n",
    "\n",
    "    # Predict and calculate log loss on the validation set\n",
    "    y_valid_pred = model.predict_proba(X_valid)[:, 1]\n",
    "    return log_loss(y_valid, y_valid_pred)\n",
    "\n",
    "# Create an Optuna study with fewer trials\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Limit to 20 trials\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best Parameters: {study.best_params}\")\n",
    "print(f\"Best Log Loss: {study.best_value:.4f}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_params = study.best_params\n",
    "final_model = lgb.LGBMClassifier(**best_params)\n",
    "final_model.fit(X_train_lgb, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_train_pred = final_model.predict_proba(X_train_lgb)[:, 1]\n",
    "train_logloss = log_loss(y_train, y_train_pred)\n",
    "print(f\"Final Training Log Loss: {train_logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
