{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client link:\n",
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "# Set up a Dask Cluster\n",
    "cluster = LocalCluster(n_workers=6, threads_per_worker=1, memory_limit='18GB')\n",
    "client = Client(cluster)\n",
    "\n",
    "print(\"client link:\")\n",
    "print(client.dashboard_link)  # Clickable link to the dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Variables for filepaths\n",
    "DATA_DIR=\"../../data\"\n",
    "\n",
    "MEMBERS_FILE=f\"{DATA_DIR}/members_v3.csv\"\n",
    "TRANSACTION_FILE=f\"{DATA_DIR}/transactions.csv\"\n",
    "TRAIN_FILE=f\"{DATA_DIR}/train.csv\"\n",
    "USERLOG_FILE=f\"{DATA_DIR}/user_logs.csv\"\n",
    "SAMPLE_SUBMISSION_FILE=f\"{DATA_DIR}/sample_submission_zero.csv\"\n",
    "\n",
    "TRANSACTION_V2_FILE=f\"{DATA_DIR}/transactions_v2.csv\"\n",
    "TRAIN_V2_FILE=f\"{DATA_DIR}/train_v2.csv\"\n",
    "USER_LOGS_V2_FILE=f\"{DATA_DIR}/user_logs_v2.csv\"\n",
    "SAMPLE_SUBMISSION_V2_FILE=f\"{DATA_DIR}/sample_submission_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: ../../data\n",
      "TRANSACTION_FILE: ../../data/transactions.csv\n",
      "USERLOG_FILE: ../../data/user_logs.csv\n",
      "TRAIN_FILE: ../../data/train.csv\n",
      "SAMPLE_SUBMISSION_FILE: ../../data/sample_submission_zero.csv\n",
      "MEMBERS_FILE: ../../data/members_v3.csv\n",
      "\n",
      "TRANSACTION_V2_FILE: ../../data/transactions_v2.csv\n",
      "USER_LOGS_V2_FILE: ../../data/user_logs_v2.csv\n",
      "TRAIN_V2_FILE: ../../data/train_v2.csv\n",
      "SAMPLE_SUBMISSION_V2_FILE: ../../data/sample_submission_v2.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"TRANSACTION_FILE: {TRANSACTION_FILE}\")\n",
    "print(f\"USERLOG_FILE: {USERLOG_FILE}\")\n",
    "print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n",
    "print(f\"SAMPLE_SUBMISSION_FILE: {SAMPLE_SUBMISSION_FILE}\")\n",
    "print(f\"MEMBERS_FILE: {MEMBERS_FILE}\")\n",
    "print()\n",
    "print(f\"TRANSACTION_V2_FILE: {TRANSACTION_V2_FILE}\")\n",
    "print(f\"USER_LOGS_V2_FILE: {USER_LOGS_V2_FILE}\")\n",
    "print(f\"TRAIN_V2_FILE: {TRAIN_V2_FILE}\")\n",
    "print(f\"SAMPLE_SUBMISSION_V2_FILE: {SAMPLE_SUBMISSION_V2_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dd.read_csv(TRAIN_FILE)\n",
    "# train = dd.concat((train, dd.read_csv(TRAIN_V2_FILE)), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "# train = dd.concat([train, dd.read_csv(TRAIN_V2_FILE)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992931"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-12 23:01:21,760 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 2e38be78d35e0d431f379b271309badf initialized by task ('shuffle-transfer-2e38be78d35e0d431f379b271309badf', 3) executed on worker tcp://127.0.0.1:33167\n",
      "2025-01-12 23:01:46,850 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 2e38be78d35e0d431f379b271309badf deactivated due to stimulus 'task-finished-1736703106.848462'\n",
      "2025-01-12 23:01:53,841 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 55a6b441a4098c9b39f7352852561079 initialized by task ('shuffle-transfer-55a6b441a4098c9b39f7352852561079', 99) executed on worker tcp://127.0.0.1:44153\n",
      "2025-01-12 23:05:20,037 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 55a6b441a4098c9b39f7352852561079 deactivated due to stimulus 'task-finished-1736703320.0292997'\n",
      "2025-01-12 23:05:46,265 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 8d25a823be6130f2569a602fed42a529 initialized by task ('shuffle-transfer-8d25a823be6130f2569a602fed42a529', 12) executed on worker tcp://127.0.0.1:36139\n",
      "2025-01-12 23:06:01,661 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 16eac19cc59c2f9c2f7b122a3e49f243 initialized by task ('shuffle-transfer-16eac19cc59c2f9c2f7b122a3e49f243', 3) executed on worker tcp://127.0.0.1:36139\n",
      "2025-01-12 23:06:21,296 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 6abaad7281efaf4343a0e44baf983fd0 initialized by task ('shuffle-transfer-6abaad7281efaf4343a0e44baf983fd0', 20) executed on worker tcp://127.0.0.1:33167\n",
      "2025-01-12 23:06:48,768 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a07e8a6a6b500e63e94268c4c486df9c initialized by task ('shuffle-transfer-a07e8a6a6b500e63e94268c4c486df9c', 0) executed on worker tcp://127.0.0.1:38701\n",
      "2025-01-12 23:06:52,137 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 8d25a823be6130f2569a602fed42a529 deactivated due to stimulus 'task-finished-1736703412.1330795'\n",
      "2025-01-12 23:06:54,016 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 16eac19cc59c2f9c2f7b122a3e49f243 deactivated due to stimulus 'task-finished-1736703414.0138319'\n",
      "2025-01-12 23:06:54,101 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle a07e8a6a6b500e63e94268c4c486df9c deactivated due to stimulus 'task-finished-1736703414.0992177'\n",
      "2025-01-12 23:07:24,295 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 6abaad7281efaf4343a0e44baf983fd0 deactivated due to stimulus 'task-finished-1736703444.2933228'\n",
      "2025-01-12 23:07:37,336 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 6d5a449bf7802b1e74a52a08433b7925 initialized by task ('shuffle-transfer-6d5a449bf7802b1e74a52a08433b7925', 18) executed on worker tcp://127.0.0.1:40445\n",
      "2025-01-12 23:08:01,821 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 6d5a449bf7802b1e74a52a08433b7925 deactivated due to stimulus 'task-finished-1736703481.8115513'\n",
      "2025-01-12 23:08:13,254 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle fd3229d46a5f869e0c303e915665df5f initialized by task ('shuffle-transfer-fd3229d46a5f869e0c303e915665df5f', 1) executed on worker tcp://127.0.0.1:38701\n",
      "2025-01-12 23:08:28,076 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle fd3229d46a5f869e0c303e915665df5f deactivated due to stimulus 'task-finished-1736703508.0727437'\n",
      "2025-01-12 23:08:37,877 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 9f9832f7107253bdf5123618dc6a780d initialized by task ('hash-join-transfer-9f9832f7107253bdf5123618dc6a780d', 3) executed on worker tcp://127.0.0.1:40445\n",
      "2025-01-12 23:08:44,933 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle ae3b7ffaf92141155db8ce842f382a66 initialized by task ('shuffle-transfer-ae3b7ffaf92141155db8ce842f382a66', 0) executed on worker tcp://127.0.0.1:40445\n",
      "2025-01-12 23:08:45,877 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle ae3b7ffaf92141155db8ce842f382a66 deactivated due to stimulus 'task-finished-1736703525.8764973'\n",
      "2025-01-12 23:08:50,827 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 945860131bdadecfb00c20e3e1731609 initialized by task ('hash-join-transfer-945860131bdadecfb00c20e3e1731609', 0) executed on worker tcp://127.0.0.1:40445\n",
      "2025-01-12 23:08:53,858 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 945860131bdadecfb00c20e3e1731609 deactivated due to stimulus 'task-finished-1736703533.856437'\n",
      "2025-01-12 23:08:53,859 - distributed.shuffle._scheduler_plugin - WARNING - Shuffle 9f9832f7107253bdf5123618dc6a780d deactivated due to stimulus 'task-finished-1736703533.856437'\n"
     ]
    }
   ],
   "source": [
    "transactions = dd.read_csv(TRANSACTION_FILE, usecols=['msno'])\n",
    "transactions = dd.DataFrame(transactions['msno'].value_counts().reset_index()).compute()\n",
    "transactions.columns = ['msno','trans_count']\n",
    "\n",
    "train = train.merge(transactions, how='left', on='msno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = dd.read_csv(TRANSACTION_FILE) \n",
    "transactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\n",
    "transactions = transactions.drop_duplicates(subset=['msno'], keep='first')\n",
    "\n",
    "train = dd.merge(train, transactions, how='left', on='msno')\n",
    "# transactions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_logs = dd.read_csv(USERLOG_FILE, usecols=['msno'])\n",
    "user_logs = dd.DataFrame(user_logs['msno'].value_counts().reset_index()).compute()\n",
    "user_logs.columns = ['msno','logs_count']\n",
    "\n",
    "\n",
    "train = dd.merge(train, user_logs, how='left', on='msno')\n",
    "# user_logs = []; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "      <th>trans_count</th>\n",
       "      <th>payment_method_id</th>\n",
       "      <th>payment_plan_days</th>\n",
       "      <th>plan_list_price</th>\n",
       "      <th>actual_amount_paid</th>\n",
       "      <th>is_auto_renew</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>membership_expire_date</th>\n",
       "      <th>is_cancel</th>\n",
       "      <th>logs_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=27</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64[pyarrow]</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64[pyarrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: merge, 12 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  msno is_churn     trans_count payment_method_id payment_plan_days plan_list_price actual_amount_paid is_auto_renew transaction_date membership_expire_date is_cancel      logs_count\n",
       "npartitions=27                                                                                                                                                                                        \n",
       "                string    int64  int64[pyarrow]             int64             int64           int64              int64         int64            int64                  int64     int64  int64[pyarrow]\n",
       "                   ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "...                ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "                   ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "                   ...      ...             ...               ...               ...             ...                ...           ...              ...                    ...       ...             ...\n",
       "Dask Name: merge, 12 expressions\n",
       "Expr=Merge(003a273)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_df(df):\n",
    "#     df = dd.DataFrame(df)\n",
    "#     df = df.sort_values(by=['date'], ascending=[False])\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "#     return df\n",
    "\n",
    "# def transform_df2(df):\n",
    "#     df = df.sort_values(by=['date'], ascending=[False])\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_user_logs = []\n",
    "# last_user_logs.append(transform_df(dd.read_csv(USER_LOGS_V2_FILE)))\n",
    "# last_user_logs = dd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "# last_user_logs = transform_df2(last_user_logs)\n",
    "# print ('merging user logs features...')\n",
    "# train = dd.merge(train, last_user_logs, how='left', on='msno')\n",
    "# test = dd.merge(test, last_user_logs, how='left', on='msno')\n",
    "# # last_user_logs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "members merge...\n"
     ]
    }
   ],
   "source": [
    "members = dd.read_csv(MEMBERS_FILE)\n",
    "train = dd.merge(train, members, how='left', on='msno')\n",
    "print('members merge...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/dask_expr/_collection.py:4225: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('gender', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "gender = {'male':1, 'female':2}\n",
    "train['gender'] = train['gender'].map(gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 492.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train = train.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del members\n",
    "del transactions\n",
    "del user_logs\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/dask_expr/_collection.py:4225: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('gender', 'float64'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 228.95 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test = dd.read_csv(TRAIN_V2_FILE)\n",
    "\n",
    "test_transactions = dd.read_csv(TRANSACTION_FILE, usecols=['msno'])\n",
    "test_transactions = dd.concat([test_transactions, dd.read_csv(TRANSACTION_V2_FILE, usecols=['msno'])])\n",
    "test_transactions = dd.DataFrame(test_transactions['msno'].value_counts().reset_index()).compute()\n",
    "test_transactions.columns = ['msno','trans_count']\n",
    "\n",
    "test = test.merge(test_transactions, how='left', on='msno')\n",
    "\n",
    "test_transactions = dd.read_csv(TRANSACTION_V2_FILE) \n",
    "test_transactions = test_transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\n",
    "test_transactions = test_transactions.drop_duplicates(subset=['msno'], keep='first')\n",
    "\n",
    "\n",
    "test = dd.merge(test, test_transactions, how='left', on='msno')\n",
    "\n",
    "\n",
    "test_user_logs = dd.read_csv(USER_LOGS_V2_FILE, usecols=['msno'])\n",
    "test_user_logs = dd.DataFrame(test_user_logs['msno'].value_counts().reset_index()).compute()\n",
    "test_user_logs.columns = ['msno','logs_count']\n",
    "\n",
    "test = dd.merge(test, test_user_logs, how='left', on='msno')\n",
    "\n",
    "members = dd.read_csv(MEMBERS_FILE)\n",
    "\n",
    "test = dd.merge(test, members, how='left', on='msno')\n",
    "\n",
    "gender = {'male':1, 'female':2}\n",
    "test['gender'] = test['gender'].map(gender)\n",
    "\n",
    "test = test.fillna(0)\n",
    "test = test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del members\n",
    "del test_transactions\n",
    "del test_user_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [ c for c in train.columns if c not in ['is_churn', 'msno']]\n",
    "\n",
    "X_train = train[cols]\n",
    "y_train = train['is_churn']\n",
    "X_test = test[cols]\n",
    "y_test = test['is_churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mean Baseline -- \n",
      "Training Accuracy: 0.9361\n",
      "Training Log Loss: 0.2376\n",
      "Test Accuracy: 0.9101\n",
      "Test Log Loss: 0.3075\n"
     ]
    }
   ],
   "source": [
    "mean_is_churn = train['is_churn'].mean()\n",
    "\n",
    "print(f\"-- Mean Baseline -- \")\n",
    "# Evaluate the model\n",
    "y_pred_prob = [mean_is_churn]*X_train.shape[0]\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "y_pred_prob = [mean_is_churn]*y_test.shape[0]\n",
    "y_pred = [1 if x > 0.5 else 0 for x in y_pred_prob]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "print(f\"Test Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple feedforward neural netwok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Feed Forward Network -- \n",
      "Training Accuracy: 0.9668\n",
      "Training Log Loss: 0.0765\n",
      "Testing Accuracy: 0.9399\n",
      "Testing Log Loss: 0.3268\n"
     ]
    }
   ],
   "source": [
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Define the MLP model in scikit-learn\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),  # Layer sizes similar to Keras\n",
    "    activation='relu',\n",
    "    solver='adam',                 # Using 'adadelta' optimizer\n",
    "    alpha=0.1,                         # L2 regularization\n",
    "    max_iter=200,                      # Number of iterations for training\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "print(f\"-- Feed Forward Network -- \")\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_train_scaled)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_train_scaled)\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "print(f\"Test Log Loss: {logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Decision Tree -- \n",
      "Training Accuracy: 0.0639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Log Loss: 0.0765\n",
      "Test Accuracy: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/shiv/e202b7b3-865c-4d22-9196-f1c9deb5d5f2/code/churn-prediction/cp_env/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Log Loss: 0.3268\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the model\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"-- Decision Tree -- \")\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_train_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_train_scaled)\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "y_pred = dt_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "print(f\"Test Log Loss: {logloss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 63471, number of negative: 929460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.086825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1004\n",
      "[LightGBM] [Info] Number of data points in the train set: 992931, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063923 -> initscore=-2.684021\n",
      "[LightGBM] [Info] Start training from score -2.684021\n",
      "-- LightGBM --\n",
      "Training Accuracy: 0.9698\n",
      "Training Log Loss: 0.0628\n",
      "Test Accuracy: 0.9387\n",
      "Test Log Loss: 0.3480\n"
     ]
    }
   ],
   "source": [
    "# Prepare LightGBM dataset\n",
    "# Ensure your dataframe is named appropriately\n",
    "X_train_lgb = X_train.copy()\n",
    "X_train_lgb['trans_count'] = X_train_lgb['trans_count'].astype('int64')\n",
    "X_train_lgb['logs_count'] = X_train_lgb['logs_count'].astype('int64')\n",
    "\n",
    "X_test_lgb = X_test.copy()\n",
    "X_test_lgb['trans_count'] = X_test_lgb['trans_count'].astype('int64')\n",
    "X_test_lgb['logs_count'] = X_test_lgb['logs_count'].astype('int64')\n",
    "\n",
    "train_data = lgb.Dataset(X_train_lgb, label=y_train)\n",
    "test_data = lgb.Dataset(X_test_lgb, label=y_test, reference=train_data)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',          # For binary classification\n",
    "    'boosting_type': 'gbdt',        # Gradient Boosting Decision Tree\n",
    "    'metric': 'binary_logloss',     # Loss metric\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "lgb_model = lgb.train(params, train_data, num_boost_round=100)\n",
    "# lgb_model = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=100, early_stopping_rounds=10)\n",
    "\n",
    "print(\"-- LightGBM --\")\n",
    "# Predict\n",
    "y_pred = lgb_model.predict(X_train_lgb)\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred_binary)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using LightGBM\n",
    "y_pred_prob = lgb_model.predict(X_train_lgb)  # This gives probabilities directly\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "y_pred = lgb_model.predict(X_test_lgb)\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using LightGBM\n",
    "y_pred_prob = lgb_model.predict(X_test_lgb)  # This gives probabilities directly\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "print(f\"Test Log Loss: {logloss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- XGBoost -- \n",
      "Training Accuracy: 0.9694\n",
      "Training Log Loss: 0.0689\n",
      "Test Accuracy: 0.9396\n",
      "Test Log Loss: 0.3430\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # For binary classification\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"-- XGBoost -- \")\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using XGBoost\n",
    "y_pred_prob = xgb_model.predict_proba(X_train)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_train, y_pred_prob)\n",
    "print(f\"Training Log Loss: {logloss:.4f}\")\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict probabilities using XGBoost\n",
    "y_pred_prob = xgb_model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "# Compute Log Loss\n",
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "print(f\"Test Log Loss: {logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:12,592] A new study created in memory with name: no-name-e1eb99a3-5314-482d-9d15-09123b82c71d\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:15,237] Trial 0 finished with value: 0.11144796531703383 and parameters: {'learning_rate': 0.014909017190986735, 'num_leaves': 42, 'min_child_samples': 13, 'n_estimators': 50, 'max_depth': 8}. Best is trial 0 with value: 0.11144796531703383.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022670 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:19,166] Trial 1 finished with value: 0.0655599425108864 and parameters: {'learning_rate': 0.09239529510397033, 'num_leaves': 27, 'min_child_samples': 13, 'n_estimators': 91, 'max_depth': 10}. Best is trial 1 with value: 0.0655599425108864.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:24,511] Trial 2 finished with value: 0.0641786844899428 and parameters: {'learning_rate': 0.0954531206571992, 'num_leaves': 31, 'min_child_samples': 24, 'n_estimators': 125, 'max_depth': 13}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:27,415] Trial 3 finished with value: 0.06767447299488068 and parameters: {'learning_rate': 0.0889721263760913, 'num_leaves': 46, 'min_child_samples': 19, 'n_estimators': 56, 'max_depth': 6}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059785 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:32,832] Trial 4 finished with value: 0.06568702063991158 and parameters: {'learning_rate': 0.05280827159751681, 'num_leaves': 41, 'min_child_samples': 14, 'n_estimators': 95, 'max_depth': 13}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:36,474] Trial 5 finished with value: 0.06972076216132696 and parameters: {'learning_rate': 0.041576737754622485, 'num_leaves': 48, 'min_child_samples': 14, 'n_estimators': 76, 'max_depth': 7}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:39,016] Trial 6 finished with value: 0.1101363301393907 and parameters: {'learning_rate': 0.013030806636634367, 'num_leaves': 28, 'min_child_samples': 15, 'n_estimators': 60, 'max_depth': 15}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:43,628] Trial 7 finished with value: 0.07090025881171172 and parameters: {'learning_rate': 0.03758256507411383, 'num_leaves': 36, 'min_child_samples': 28, 'n_estimators': 118, 'max_depth': 5}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:49,841] Trial 8 finished with value: 0.06900224888649138 and parameters: {'learning_rate': 0.023567013424302813, 'num_leaves': 41, 'min_child_samples': 23, 'n_estimators': 145, 'max_depth': 8}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026500 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:51,982] Trial 9 finished with value: 0.08250259787217804 and parameters: {'learning_rate': 0.05547308617742812, 'num_leaves': 49, 'min_child_samples': 24, 'n_estimators': 77, 'max_depth': 2}. Best is trial 2 with value: 0.0641786844899428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:21:56,970] Trial 10 finished with value: 0.07396438379179772 and parameters: {'learning_rate': 0.024133906381713404, 'num_leaves': 20, 'min_child_samples': 30, 'n_estimators': 124, 'max_depth': -1}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:01,676] Trial 11 finished with value: 0.06491662519132428 and parameters: {'learning_rate': 0.09738516128402146, 'num_leaves': 28, 'min_child_samples': 10, 'n_estimators': 110, 'max_depth': 11}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:07,044] Trial 12 finished with value: 0.06514120515298778 and parameters: {'learning_rate': 0.06886984350762432, 'num_leaves': 30, 'min_child_samples': 19, 'n_estimators': 119, 'max_depth': 12}. Best is trial 2 with value: 0.0641786844899428.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:12,916] Trial 13 finished with value: 0.06353930947710773 and parameters: {'learning_rate': 0.09767443321673915, 'num_leaves': 33, 'min_child_samples': 24, 'n_estimators': 139, 'max_depth': 15}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:19,460] Trial 14 finished with value: 0.06386573464208979 and parameters: {'learning_rate': 0.06467960621585452, 'num_leaves': 34, 'min_child_samples': 25, 'n_estimators': 146, 'max_depth': 15}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:25,963] Trial 15 finished with value: 0.06375764657325793 and parameters: {'learning_rate': 0.0615710769528543, 'num_leaves': 36, 'min_child_samples': 26, 'n_estimators': 149, 'max_depth': 15}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:31,921] Trial 16 finished with value: 0.06402176274025845 and parameters: {'learning_rate': 0.06925298960970899, 'num_leaves': 36, 'min_child_samples': 21, 'n_estimators': 135, 'max_depth': 15}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:37,934] Trial 17 finished with value: 0.07012284416810646 and parameters: {'learning_rate': 0.03035966559304507, 'num_leaves': 22, 'min_child_samples': 27, 'n_estimators': 150, 'max_depth': 10}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028338 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:42,165] Trial 18 finished with value: 0.07523542894623725 and parameters: {'learning_rate': 0.047202156679456465, 'num_leaves': 38, 'min_child_samples': 21, 'n_estimators': 136, 'max_depth': 3}. Best is trial 13 with value: 0.06353930947710773.\n",
      "/tmp/ipykernel_36972/269045056.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 50866, number of negative: 743478\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1012\n",
      "[LightGBM] [Info] Number of data points in the train set: 794344, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064035 -> initscore=-2.682144\n",
      "[LightGBM] [Info] Start training from score -2.682144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 01:22:47,743] Trial 19 finished with value: 0.0657677506224352 and parameters: {'learning_rate': 0.07213076928707594, 'num_leaves': 24, 'min_child_samples': 27, 'n_estimators': 136, 'max_depth': 13}. Best is trial 13 with value: 0.06353930947710773.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.09767443321673915, 'num_leaves': 33, 'min_child_samples': 24, 'n_estimators': 139, 'max_depth': 15}\n",
      "Best Log Loss: 0.0635\n",
      "[LightGBM] [Info] Number of positive: 63471, number of negative: 929460\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1004\n",
      "[LightGBM] [Info] Number of data points in the train set: 992931, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063923 -> initscore=-2.684021\n",
      "[LightGBM] [Info] Start training from score -2.684021\n",
      "Final Training Log Loss: 0.0612\n",
      "Final Test Log Loss: 0.3646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define a reduced hyperparameter search space\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),  # Focused range\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),                 # Reduced range\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 30),   # Smaller range\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),            # Moderate boosting rounds\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 15),                   # Practical range\n",
    "    }\n",
    "\n",
    "    # Split data for validation\n",
    "    X_train_part, X_valid, y_train_part, y_valid = train_test_split(\n",
    "        X_train_lgb, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model = lgb.LGBMClassifier(**param)\n",
    "    # model = model.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "    model.fit(X_train_part, y_train_part, eval_set=[(X_valid, y_valid)])\n",
    "            #   eval_metric='logloss',  verbose=0) # early_stopping_rounds=10,\n",
    "\n",
    "    # Predict and calculate log loss on the validation set\n",
    "    y_valid_pred = model.predict_proba(X_valid)[:, 1]\n",
    "    return log_loss(y_valid, y_valid_pred)\n",
    "\n",
    "# Create an Optuna study with fewer trials\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)  # Limit to 20 trials\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best Parameters: {study.best_params}\")\n",
    "print(f\"Best Log Loss: {study.best_value:.4f}\")\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_params = study.best_params\n",
    "final_model = lgb.LGBMClassifier(**best_params)\n",
    "final_model.fit(X_train_lgb, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_train_pred = final_model.predict_proba(X_train_lgb)[:, 1]\n",
    "train_logloss = log_loss(y_train, y_train_pred)\n",
    "print(f\"Final Training Log Loss: {train_logloss:.4f}\")\n",
    "\n",
    "# Evaluate the final model\n",
    "y_test_pred = final_model.predict_proba(X_test_lgb)[:, 1]\n",
    "test_logloss = log_loss(y_test, y_test_pred)\n",
    "print(f\"Final Test Log Loss: {test_logloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# report\n",
    "```\n",
    "-- Mean Baseline -- \n",
    "Train Accuracy: 0.9361\n",
    "Train Log Loss: 0.2376\n",
    "Test Accuracy: 0.9101\n",
    "Test Log Loss: 0.3075\n",
    "\n",
    "-- Feed Forward Network -- \n",
    "Train Accuracy: 0.9668\n",
    "Train Log Loss: 0.0765\n",
    "Test Accuracy: 0.9399\n",
    "Test Log Loss: 0.3268\n",
    "\n",
    "-- Decision Tree -- \n",
    "Train Accuracy: 0.0639\n",
    "Train Log Loss: 0.0765\n",
    "Test Accuracy: 0.0899\n",
    "Test Log Loss: 0.3268\n",
    "\n",
    "-- LightGBM --\n",
    "Train Accuracy: 0.9698\n",
    "Train Log Loss: 0.0628\n",
    "Test Accuracy: 0.9387\n",
    "Test Log Loss: 0.3480\n",
    "\n",
    "-- XGBoost -- \n",
    "Train Accuracy: 0.9694\n",
    "Train Log Loss: 0.0689\n",
    "Test Accuracy: 0.9396\n",
    "Test Log Loss: 0.3430\n",
    "\n",
    "-- Tunned LightGBM --\n",
    "Train Log Loss: 0.0612\n",
    "Test Log Loss: 0.3646\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(970960,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moRTKhKIDvb+C8ZHOgmaF4dXMLk0jOn65d7a8tQ2Eds=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5rqTxCnG7s5VBgEfdkQCezv5KBK7+DMujNibYgylrs=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1AzXWFlRO6EfMBzfBdk98sBVnjzY7U1G24mVFNdzGNQ=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qpV8BYuYz/Z7LFqEuo2QEMfwWWxdCIQQT4X/XsPXwhc=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LZjqFj4TwHsByrOSyjUp9l/B9WOF34HGX0Hx7uiQ5xQ=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162517</th>\n",
       "      <td>PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162518</th>\n",
       "      <td>MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162519</th>\n",
       "      <td>SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162520</th>\n",
       "      <td>eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162521</th>\n",
       "      <td>/QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                msno  is_churn\n",
       "0       moRTKhKIDvb+C8ZHOgmaF4dXMLk0jOn65d7a8tQ2Eds=         1\n",
       "1       t5rqTxCnG7s5VBgEfdkQCezv5KBK7+DMujNibYgylrs=         1\n",
       "2       1AzXWFlRO6EfMBzfBdk98sBVnjzY7U1G24mVFNdzGNQ=         1\n",
       "3       qpV8BYuYz/Z7LFqEuo2QEMfwWWxdCIQQT4X/XsPXwhc=         1\n",
       "4       LZjqFj4TwHsByrOSyjUp9l/B9WOF34HGX0Hx7uiQ5xQ=         1\n",
       "...                                              ...       ...\n",
       "162517  PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=         0\n",
       "162518  MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=         0\n",
       "162519  SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=         0\n",
       "162520  eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=         0\n",
       "162521  /QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=         0\n",
       "\n",
       "[970960 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = test[['msno', 'is_churn']].copy()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['is_churn'] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msno</th>\n",
       "      <th>is_churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moRTKhKIDvb+C8ZHOgmaF4dXMLk0jOn65d7a8tQ2Eds=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5rqTxCnG7s5VBgEfdkQCezv5KBK7+DMujNibYgylrs=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1AzXWFlRO6EfMBzfBdk98sBVnjzY7U1G24mVFNdzGNQ=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qpV8BYuYz/Z7LFqEuo2QEMfwWWxdCIQQT4X/XsPXwhc=</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LZjqFj4TwHsByrOSyjUp9l/B9WOF34HGX0Hx7uiQ5xQ=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162517</th>\n",
       "      <td>PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162518</th>\n",
       "      <td>MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162519</th>\n",
       "      <td>SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162520</th>\n",
       "      <td>eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162521</th>\n",
       "      <td>/QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970960 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                msno  is_churn\n",
       "0       moRTKhKIDvb+C8ZHOgmaF4dXMLk0jOn65d7a8tQ2Eds=         0\n",
       "1       t5rqTxCnG7s5VBgEfdkQCezv5KBK7+DMujNibYgylrs=         0\n",
       "2       1AzXWFlRO6EfMBzfBdk98sBVnjzY7U1G24mVFNdzGNQ=         0\n",
       "3       qpV8BYuYz/Z7LFqEuo2QEMfwWWxdCIQQT4X/XsPXwhc=         1\n",
       "4       LZjqFj4TwHsByrOSyjUp9l/B9WOF34HGX0Hx7uiQ5xQ=         0\n",
       "...                                              ...       ...\n",
       "162517  PsftdQEI+bQFl8FB2+O4sKM4uRZGO/UvBCDS+ZyWmvk=         0\n",
       "162518  MqyOPaDM7Jz3kV3fu/h9ilHP3TxLaMFE9raYkEHg5Jg=         0\n",
       "162519  SjVZDYaiKgEHpFX1PcFDS94b9CFdaHjg78rfumtm/F4=         0\n",
       "162520  eZ3y0lsY2SVZc2h8T3zB454TuBz6oVDMlFsEpEPQclQ=         0\n",
       "162521  /QlhSIWEZelYnwttYJSQL50EJJ2yRV+ThQIVQLvctp8=         0\n",
       "\n",
       "[970960 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
